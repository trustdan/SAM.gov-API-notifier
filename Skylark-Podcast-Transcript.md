## **Speakers**

It's pretty fascinating, isn't it, watching how AI is

weaving itself into more and more of our lives. It really is. But you know, when you really stop and think about it, so much of the AI we have now,

it still needs these like massive data sets and constant human tweaking, yeah, constant input. It really makes you wonder, what if AI could learn more like we do, you know, constantly adapting, figuring things out on its own. That's pretty much the core question for our deep dive. Today, we're looking into some really

groundbreaking work coming out of skylark labs. Skylark labs, okay, yeah, they're developing something they call brain inspired hybrid deep networks. It's

a totally different way of thinking about AI and a different approach, fundamentally different. Their goal is, well, it's ambitious. They talk about creating digital life forms. Whoa, digital life forms. Okay, that definitely sounds like something from a movie. It does. It does, but it's clearly an idea gaining serious traction. You hear Nvidia's Jensen Huang talking about physical AI as like the next trillion dollar frontier, right? I've heard that. And Dr Amarjot Singh skylark CEO, he's right there echoing that sentiment. So there's real weight behind this, exactly. And our mission, you know, for this deep dive, is to really unpack what skylark Labs is trying to pull off here. How are they aiming to get past the current limits of AI? How do they build these truly adaptive, collaborative systems? Okay, we've got some really interesting source material articles digging into their their unique architecture. How they get AI to learn together, their work on giving AI a kind of self awareness, self awareness in AI, okay, yeah, and even how this whole thing could potentially lead down a path towards safe, super intelligence. Wow. Okay, so we're really trying to get under the hood here, understand how skylark is building AI that doesn't just crunch data, but actually learns and grows almost organically. That's the plan. Hopefully uncover some aha moments about how they're tackling this incredibly complex challenge. All right, let's dive in. Then these digital life forms. What exactly do they mean by that? What's the core vision? So the core vision is about creating these Well, digital entities that can essentially live and work in the physical world right alongside us live and work. Yeah, and the key part is adapting, adapting in real time to new stuff, new challenges, without us needing to constantly step in, okay, without the hand holding Exactly. Dr Singh draws a sharp contrast with current AI. He argues pretty convincingly. I think that most AI today is really only as good as its last training data set right. Hits a wall when it sees something new precisely and updating it, teaching it new concepts. That often means expensive, time consuming, retraining, usually done by humans. Okay, so that dependence on us, that's the bottleneck they're trying to break. That's the fundamental limitation. Yeah, they're shifting the focus. It's less about making machines that think exactly like humans, and more about making machines that learn like humans, continuously, autonomously, picking things up, learning like humans, that continuous process Exactly. Dr, Singh stress is, you know, their goal isn't just tweaking algorithms. It's creating systems that learn from their own experiences, adapt when things get weird or unexpected, without us constantly needing to intervene, right? And these digital life forms as they see them, they could operate in dangerous places, handle repetitive stuff, and this is key, actually collaborate with us on complex problems. It's about creating truly autonomous intelligence. It makes you picture things, though. What would these actually look like? Are we talking? See through po walking around? Yeah. Well, humanoid robots are definitely one possibility, sure, especially for service rules. Maybe okay, but Dr Singh is clear their intelligence, their system isn't locked into one body shape,

their brain inspired architecture is designed to be flexible, so you could see this intelligence in autonomous drones doing aerial work, or maybe those four legged robots for search and rescue, like the Boston Dynamics ones, kind of, yeah, or even highly specialized industrial robots, the core ideas they put it is, form follows function makes sense. The body fits the job, right. The physical shell is optimized for whatever environment and tasks it needs to handle. Okay, so the big picture, AI that adapts lives in the real world takes different shapes. What's the secret sauce? Then? How are they actually building this AI differently? What's the core tech? The the fundamental difference is in their architecture, these brain inspired hybrid deep networks. Okay, see, traditional AI often uses like a uniform structure, same learning methods for pretty much everything. One size fits all, sort of Yeah, but skylarks approach mirrors the brain more closely, yeah. Our brains have specialized areas, right, different regions for different jobs, visual cortex, auditory cortex, exactly their AI does something similar. It has specialized regions within the network using diverse learning.

Principles. This lets different abilities kind of emerge naturally, naturally, yeah, like different parts of our brain learn in different ways. Think of it like a school with specialized departments, physics, art, language, instead of one teacher trying to cover everything, skylarks. AI has these departments that get really good at specific types of learning. And this connects back to Dr Singh's background right, Cambridge, Stanford, his work with DARPA, absolutely, especially the DARPA lifelong learning machines program that was all about pushing AI towards continuous learning, taking skills learned in one place and applying them somewhere new, and crucially avoiding what's called catastrophic forgetting. Ah, yes, where learning something new wipes out old knowledge, exactly. And their hybrid learning paradigm is key to achieving this. It's sort of layered. It starts based the AI learns to spot simple features like lines in an image, almost like a baby seeing basic shapes, okay, low level stuff first, right? Then it moves to unsupervised learning. It starts grouping those features into more complex patterns on its own, curves, textures, without needing explicit labels for everything, learning patterns without being told what they are exactly. And then finally, supervised learning comes in to classify meaningful objects based on those learned patterns. Like, okay, this specific combo of shapes and textures that's a cat. Got it step by step, and this layered approach is way more efficient. Needs less data, less computing power, which means, and this is important, these sophisticated models can actually run on smaller devices, edge devices, things with limited resources, like microcontrollers, standard CPUs, not just massive server farms. That sounds incredibly practical, and this whole specialized structure, this learning process, it leads to those three key brain inspired abilities they talk about, yes, and these are really what make their approach stand out. First is self awareness, okay? Or metacognition. Is the technical term. It means the AI basically understands its own limits. It can recognize when it doesn't have enough information to be sure about something. It knows what it doesn't know. That's the analogy Dr Singh uses, yeah, like humans who know when they don't know something. And that's huge, right? Yeah, it stops the AI from just confidently making a bad call based on shaky understanding that alone feels like a massive leap. What's number two second is self evolution. This is more than just tweaking parameters their AI networks can actually like, restructure themselves, change their own internal wiring in real time when they encounter something totally new, wait, rewrite their own code basically, sort of, yeah. Imagine Software that optimizes itself on the fly for a new kind of problem. It lets the AI adapt to novel concepts without needing humans to take it offline, rebuild it, retrain it. It's constantly refining how it processes the world. Wow. Okay, so not just learning what's new, but changing how it learns That's deep. And the third capability, the third is collaborative teaching, and this is where it gets really fascinating. Really mirrors how we learn from each other, collaborative teaching like aI school, huh? Sort of it's inspired by our mirror neuron system, you know, the brain cells that fire both when we do something and when we watch someone else do it, yeah, learning by watching Exactly. So one skylark AI system can directly share what it's learned with another skylark AI, and it does this without needing to send huge data sets back and forth or go through long retraining cycles. Oh, it's almost like one AI can just show another AI. Hey, here's how you recognize this new thing, or here's how you handle this new situation. So it's not just about smart individuals, but a collective intelligence they can teach each other. That's huge for like, distributed systems, massive implications, yeah, but this all sounds very futuristic. Is any of this actually out? There are these digital life forms solving real problems now? Oh, absolutely. Skylarks tech this embodied intelligence. It's already deployed in some pretty critical areas. Okay? Their systems are built to fuse data all sorts of sensors, cameras, audio, radar, LiDAR, multiple senses, right? And use that fuse picture to predict threats and, crucially, keep learning in environments where things change fast and maybe humans can't always be watching, okay, give us some examples. Where is this tech in action? Well, they have these Scout AI towers used for border monitoring. They're constantly looking for new patterns, anomalies, things that don't fit border security. Then there's their a rise drone detection platform that's been shown to the US Department of Defense. Yeah, it can spot and track drones even when they're too far away to see beyond visual line of sight. And army, Colonel, retired Colonel Brad Boyd spoke very highly of its capabilities. There impressive. What else? They also have a mobile detection platform for Transport Safety, like on roads or railways safety guard, AI for urban security. And maybe one of the most impactful uses is called Sensor. Sensor. Yeah, it's been used in India to help find and rescue children who've been trafficked. Yeah.

Even if their appearance has changed a lot over time, the AI can still help identify them. Wow, that's that really hits home. The positive potential there is huge. It really is. So these systems aren't just sitting there, they're actively learning, adapting to the specific challenges in these tough, real world places exactly. Okay, let's circle back to that collaborative learning that sounds revolutionary. How does one AI actually share its knowledge with another one? What's the mechanism, right? This is their collaborative learning framework, and it is pretty innovative. It enables direct machine to machine knowledge transfer,

and crucially, without needing a constant Cloud connection or any human stepping in. It really mimics how humans share insights, learn from each other's experiences. Unlike traditional AI, where each system learns on its own Pretty much, yeah, right, traditional systems, even if they're networked, often learn in isolation, yeah? Which is really inefficient, especially when you need to share info about, say, a brand new threat that just popped up, right? So if one AI on the border spots a new smuggling technique, traditionally, that knowledge might just sit there until a person notices and manually updates all the other systems. That's often the case. Yeah, skylarks framework, though, takes inspiration for those mirror neurons. We mentioned the learning by observation thing, exactly those cells firing when we watch someone else act. That's key for observational learning. Their system basically replicates that. It lets one AI transfer a new skill or piece of knowledge to another, almost just by the second AI observing what the first one learned. No explicit instructions needed, no massive data dumps. That's a really cool biological parallel. Okay, technically, though, how does it work? How does the knowledge actually get transferred? It's a clever process, sometimes called Knowledge distillation. It happens in steps. First is knowledge extraction, the AI that learns something new creates a simplified, abstract summary of that knowledge, like a teacher boiling down a complex topic into key bullet points, essence of the learning exactly. Then comes knowledge transmission, that abstract summary gets compressed into a tiny file, sometimes just kilobytes, kilobytes. Wow, that's small. Super small makes it easy to share, even if the connection is slow or spotty, like low bandwidth radio, right? No need for massive downloads. And finally, the really crucial step knowledge integration, the AI receiving this tiny knowledge packet, takes it and folds it into its own understanding without messing up or forgetting what it already knew. This tackles that catastrophic forgetting problem head on. Okay, so they're not sharing raw data, not even whole AI models, just the distilled wisdom, basically in a tiny package that explains the speed and efficiency precisely. What kind of results are they seeing from this in the real world? What are the benefits? The results are pretty striking for border security. With their Scout MK Thurs and towers, they reported an 83% cut in the time it takes for the whole network to adapt to new smuggling methods. 83% reduction. That's huge. Yeah, what might have taken weeks of human analysis and manual updates can now happen across the network in minutes automatically. A former Border Patrol agent, Philip Koch, talked about how much this boosted their operational effectiveness, I can imagine. And similarly, for counter drone work with the Ares platform in exercises different ares units learned collaboratively. They figured out how to spot new drone types adapt to new evasion tactics, all within minutes and without needing the cloud. Exactly no cloud dependency for the learning transfer itself. Colonel Brad Boyd saw this firsthand, how quickly they could adapt collectively, that kind of rapid shared adaptation in security scenarios. That sounds incredibly valuable. You also mentioned cross modal transfer. What's that about? Yeah, that's another really powerful angle, because the knowledge is abstracted, right? It's not tied to one specific sensor type, okay? So an AI using mainly vision, a camera can actually share its understanding of, say, a threat pattern with another AI that's using totally different sensors like radar or audio. Oh, really, yeah, the receiving AI takes that abstract knowledge and figures out how to apply it using its own senses. In one demo, a visual AI spotted a specific unauthorized access pattern, and it successfully taught a nearby radar system to spot the same pattern using radar signals, which are obviously completely different from visual data. So an AI that sees can teach an AI that hears or uses radar to recognize the same kind of threat. That's remarkable flexibility, it really is. So boiling it down, what are the main operational wins from this collaborative approach. Oh, huge reduction in the need for manual retraining by humans. That saves a lot of time and money. Skylark estimates cost cuts over 60% in some scenarios, significant savings, definitely. But arguably more important is the speed and effectiveness of response. New threats don't slip through the cracks for weeks just because they're novel.

As Dr Singh puts it, something detected by one AI can be known by the whole network in minutes. That's a game changer, and their future focus is even pushing towards collaborative problem solving, where multiple AIS could team up to tackle complex challenges that are too big for any single AI. This really paints a picture of AI that's not just smart individually, but collectively intelligent, like a constantly learning, adapting network brain. That's a good way to put it. Okay, let's shift gears slightly to that self awareness piece you mentioned earlier. That sounds incredibly tricky. How does an AI actually know what it doesn't know? That feels so human. It is a major step forward. Yeah, you know how sometimes traditional AI can fail,

but fail confidently? Yeah, it gives you a wrong answer, but acts like it's 100% certain. The Confident failure problem exactly, might misclassify something totally new or miss it entirely, but give no indication it's uncertain. That can be dangerous, right? Skylarks AI, though, is built with this sort of self awareness. It can recognize the edges of its own knowledge, kind of like how we know when we're out of our depth on a topic, okay, how does that work? In practice, you mentioned Dr Singh's example with the weird aircraft, right? The maritime surveillance footage showed standard patrol boats, attack helicopters, things the AI knew. But then there was this unusual amphibious aircraft of VTL vertical takeoff and landing looks sort of like a helicopter, sort of like a boat, something it hadn't been trained on exactly a traditional AI might have just guessed helicopter or ignored it, but skylarks AI analyzed its parts. It saw rotors like a helicopter. It saw a hull like a boat. It understood the components, yes, because it understood those fundamental building blocks, it could recognize, hey, this combination is new, but it has features that seem relevant. This is potentially important, even though it had never seen that specific aircraft before. So it didn't just guess. It flagged it as novel and noteworthy. Precisely, it's not just matching whole patterns from its training data. It's understanding the basic features and how they relate structurally, deeper understanding, not just pattern matching, right? As Dr Singh explained, even if it's never seen that specific hybrid plane, knowing what rotors are, what hulls are, allows it to reason that this new object warrants attention. That ability to spot both the known and the unknown threats, is obviously critical and dynamic situation, and that self awareness is the key to getting past that confident failure issue. Absolutely because it knows its limits, it avoids making those confident mistakes on unfamiliar things. Instead, it flags them. This is novel, maybe important, that triggers more analysis or learning instead of just plowing ahead with a bad guess. This capability is really central to their Kepler platform. Kepler and this is being used effectively now, yes, in correctional facilities, for instance, Kepler's help significantly increase the detection of improvised weapons, because it can spot unusual objects, things that don't look like known contraband, but have suspicious shapes or features. It flags them because they're novel but potentially concerning. They reported a 78% jump in detection rates, 78%

wow and in border security, it's been good at identifying new concealment methods. In maritime tests, it consistently spotted all the legitimate vessels, even ones with weird new designs that other AIS missed, so it doesn't miss the important stuff. That's what Reddit geology Lieutenant Commander Sarah Chen emphasized. The crucial thing is it never misses vessels that matter. That's a powerful endorsement. And this self awareness, it's tied into their idea of continuous on device learning too, right? Exactly. They go hand in hand, because the AI can spot new, unfamiliar patterns. Using its self awareness, it can then start learning about those new patterns right there on the device itself. No need to phone home to the cloud, no need for massive new data sets or a full retraining cycle learning on the fly, kind of like the hippocampus in our brain, which helps form new short term memories. Yeah. Dr Singh uses that analogy, the AI holds these new patterns temporarily, okay, if the pattern doesn't show up again, if it was just a one off, weird thing, the AI eventually forgets it, like we forget random things we saw once filters out the noise, right? But if that new pattern keeps reappearing, it gets reinforced. It gets consolidated into the AI's longer term memory, its core understanding similar to how our hippocampus and neocortex work together to solidify important memories. So it's not just learning, it's learning what's important to remember efficiently. We've heard some examples of this self evolution in action too. We have Kurt PK Dube shared an experience at a military base where Kepler spotted a totally new kind of drone. This drone was using clever tricks to avoid normal radar. Sneaky drone, yeah, but Kepler picked it up based on its weird flight pattern, its unique signature, and then it adapted its own detection methods in real time to track it better. Wow. Adapted on the spot exactly, and their tracer a.

I mobile platform that's used for runway safety, looking for foreign object debris, fod, stuff left on the runway that could damage planes. Right, curl career mentioned that the type of FOD changes all the time. Tracer learns as it drives along the runway, adapting to identify new kinds of debris without needing manual updates, signing on the move, and you can see the potential right pick self driving cars adapting instantly to unexpected road closures or debris based on what their own sensors are seeing. Dr Andrea saltogio also pointed out that this approach seems to be genuinely tackling that old AI challenge, the stability plasticity dilemma, meaning it can learn new things plasticity without catastrophically forgetting the important old stuff, stability. It balances learning with remembering. Okay, that makes sense. This continuous learning self evolution really feels like the building blocks for something more advanced. Which brings us to their bigger vision, safe, super intelligence in these digital life forms. How do they connect the dots from here to there. Yeah, it's a big leap, but their thinking is that true super intelligence, AI that genuinely surpasses human abilities across the board, probably won't just pop out of nowhere from one single invention. Okay? Instead, they argue it's more likely to emerge from the combination of these two things, continuous adaptation, the ability to learn and evolve constantly. Right and collaborative knowledge sharing, the ability for AIS to teach and learn from each other rapidly. These are the exact capabilities their brain inspired tech is designed for. So it's the synergy between adapting and collaborating that leads to intelligence explosion. That's the idea they see a fundamental like qualitative difference between their dynamic, evolving, connected systems and the more static, isolated traditional AI, they compare it to watching a live video versus just looking at a single photo. The photo is frozen, the video is constantly changing and updating. Exactly that constant learning amplified by sharing that learning instantly across the network. They see that creating a kind of super intelligence multiplier effect learning could potentially become exponential, okay, an exponential learning curve driven by adaptation and collaboration. What are the key tech pieces enabling this? According to them, they highlight a few things that memory based, metacognition, the self awareness piece, which lets the AI, quantify its own uncertainty, know when to seek more info right then the neuromorphic knowledge representation, that's what allows the super efficient compression and sharing of knowledge in the tiny kilobytes, packets exactly and cross domain transfer learning, the ability to take knowledge learned in one area like visual recognition and apply it effectively in a totally different area, Like radar analysis, putting it all together. And they've actually laid out a kind of roadmap how they see this unfolding. Yeah. They have a phase plan looking ahead. Phase one, where they see themselves now, is about building these distributed perception networks, AI networks, that can sense and understand the world in a distributed way. Okay, current state. Phase two, they're calling autonomous strategy formation that's projected for maybe 2025 2026

the idea is these networks could collectively come up with novel solutions to complex problems. AI is collaborating on strategy right then phase three, cross domain understanding around 2026, 2027

that's where knowledge truly flows freely between different operational areas. Okay? And finally, phase four, targeting 2027 2028 is human machine collaborative intelligence, aiming for really seamless integration between human thinking and AI capabilities. That's ambitious, a very clear vision for the next few years. And throughout this they're stressing safety by design, right? How are they approaching that? Absolutely safety seems to be a core principle, not an afterthought. They talk about building in ethical boundary models from the start guard roll exactly, ensuring transparent reasoning. So we can actually understand why the AI made a certain decision, continuously verifying value alignment, making sure its goals stay aligned with ours, right, and using graduated autonomy, slowly increasing the AI's independence while keeping humans in the loop and able to oversee gradual rollout of capabilities. Yeah, they keep hammering this point. The goal is human digital symbiosis, not replacement. Working with AI not being replaced by it. That's the vision, augmented decision making, collaborative problem solving, where humans and AI tackle things together, maybe even eventually some form of cognitive integration, but preserving our distinct identities. Dr, Singh is very clear the path that digital life forms has to be responsible with ongoing ethical checks and balances, ensuring these powerful non biological entities stay aligned with human well being and values. Okay, so wrapping this up, this deep dive into skylark labs, really shows a different path for AI, doesn't it? This brain inspired approach, aiming for digital life forms that learn, continuously, adapt on their own, collaborate,

feels like a significant shift from the mainstream.

It really does that brain inspired hybrid architecture seems to be the key unlocking these advanced abilities like self awareness and that powerful collaborative learning and importantly, building safety in from the ground up as they push towards more advanced systems. It definitely leaves you with a lot to chew on. You know, think about the implications AI systems that can genuinely learn and evolve on their own and together. How does that fundamentally change our relationship with technology over the next, say, 5-10, years? It's a huge question. Yeah, what new possibilities open up, but also, what new challenges arise when these digital life forms start acting with more autonomy in well, potentially, every aspect of our lives? It's certainly a future we need to keep exploring and thinking about very carefully indeed, the ideas we've covered today, they offer a pretty fascinating peek into a potential future of human AI coexistence, one that goes way beyond what most AI can do today. We definitely encourage you, the listener, to mull over these concepts and consider the big questions raised by this push towards more autonomous, adaptive intelligence in the world around us.
